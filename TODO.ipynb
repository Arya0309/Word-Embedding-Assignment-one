{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(file_path, preprocessed_file):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        with open(preprocessed_file, \"w\", encoding=\"utf-8\") as out:\n",
    "            for line in tqdm(f, desc=\"Preprocessing\", total=len(f)):\n",
    "                tokens = preprocess(line)\n",
    "                out.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "sample_percentage = 0.01\n",
    "sample_file = f\"sample_{sample_percentage}.txt\"\n",
    "preprocessed_file = \"preprocessed_corpus.txt\"\n",
    "chunk_size = 1000\n",
    "\n",
    "def preprocess(line):\n",
    "    tokens = gensim.utils.simple_preprocess(line, deacc=False, min_len=2, max_len=15)\n",
    "    return tokens\n",
    "\n",
    "def parallel_preprocess(file_path, total_lines, output_file, pool_size=None, chunk_size=1000):\n",
    "    if pool_size is None:\n",
    "        pool_size = cpu_count()\n",
    "\n",
    "    with Pool(processes=pool_size) as pool:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            lines = []\n",
    "            for i, line in enumerate(tqdm(infile, total=total_lines, desc=\"Preprocessing\")):\n",
    "                lines.append(line)  # remove any extra whitespace or newline characters\n",
    "                \n",
    "                if (i + 1) % chunk_size == 0:\n",
    "                    tokens_chunk = pool.map(preprocess, lines)\n",
    "                    for tokens in tokens_chunk:\n",
    "                        outfile.write(\" \".join(tokens) + \"\\n\")\n",
    "                    outfile.flush()  # ensure the data is written to file\n",
    "                    lines = []\n",
    "\n",
    "            if lines:  # process remaining lines\n",
    "                tokens_chunk = pool.map(preprocess, lines)\n",
    "                for tokens in tokens_chunk:\n",
    "                    outfile.write(\" \".join(tokens) + \"\\n\")\n",
    "                outfile.flush()\n",
    "\n",
    "class TextCorpus:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return sum(1 for _ in f)\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(f, total=total_lines, desc=\"Training\"):\n",
    "                yield line\n",
    "\n",
    "# Calculate total lines in the corpus file\n",
    "total_lines = len(TextCorpus(sample_file))\n",
    "\n",
    "# Preprocess the file in parallel\n",
    "parallel_preprocess(sample_file, total_lines, preprocessed_file, pool_size=cpu_count(), chunk_size=chunk_size)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
